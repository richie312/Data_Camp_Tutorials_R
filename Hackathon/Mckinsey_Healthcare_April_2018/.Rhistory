test = read.csv("test.csv", stringsAsFactors = FALSE)
head(train)
head(test)
factor_list<- c("gender","hypertension","heart_disease","ever_married","work_type","Residence_type",
"smoking_status","stroke")
train[,factor_list] <- lapply(train[,factor_list], factor)
str(train)
## Convert the variables in test set
factor_list_test<- c("gender","hypertension","heart_disease","ever_married","work_type","Residence_type",
"smoking_status")
test[,factor_list_test]<-lapply(test[,factor_list_test], factor)
set.seed(1)
## Train the model
train_model<- randomForest(stroke~.,data = train, na.action = na.omit)
## Print the model output
print(train_model)
## Evaluate Out of Bag error
err<- train_model$err.rate
## Look at the final OOB rate
oob_err<-err[nrow(err), "OOB"]
print(oob_err)
## Plot the train model
plot(train_model)
# Add a legend since it doesn't have one by default
legend(x = "right",
legend = colnames(err),
fill = 1:ncol(err))
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
## Create the dataframe for sample file
colnames(prob_prediction)<- c("Not Happening", "Happening")
sample<-cbind(test$id,prob_prediction$Happening)
sample
write.csv(sample,"sample_submission.csv")
write.csv(sample,"sample_submission.csv")
colnames(sample)<- c("id", "stroke")
head(sample)
head(prob_prediction)
colnames(prob_prediction)<- c("Happening", "Not Happening")
head(prob_prediction)
rm(sample)
sample<-cbind(test$id,prob_prediction$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
write.csv(sample,"sample_submission.csv")
head(sample)
index<- which(is.na(train), arr.ind = TRUE)
index
is.na(train$gender)
any(is.na(train$gender))
any(which(train[,factor_list]))
any(is.na(train[,factor_list]))
index<-which(is.na(train), arr.ind = TRUE)
index<- which(is.na(train), arr.ind = TRUE)
index<- which(is.na(train), arr.ind = TRUE)
str(train)
is.na(train$age)
any(is.na(train$age))
str(train)
train[indx] <- rowMeans(train, na.rm = TRUE)[indx[,"row"]]
any(is.na(train$id))
train[index] <- rowMeans(train, na.rm = TRUE)[index[,"row"]]
index<-which(is.na(train), arr.ind = TRUE)
train[index] <- rowMeans(train, na.rm = TRUE)[index[,"row"]]
train[index]
any(is.na(train$bmi))
any(is.na(train$avg_glucose_level))
nrow(is.na(train$bmi))
is.na(train$bmi)
is.na(train$bmi)  == TRUE
train$bmi[is.na(Test_Data$bmi)] <- mean(train$bmi,na.rm=T)
train$bmi[is.na(train$bmi)] <- mean(train$bmi,na.rm=T)
any(is.na(train))
rm(train_model)
rm(prob_prediction)
rm(sample)
set.seed(1)
## Train the model
train_model<- randomForest(stroke~.,data = train)
## Print the model output
print(train_model)
## Evaluate Out of Bag error
err<- train_model$err.rate
## Look at the final OOB rate
oob_err<-err[nrow(err), "OOB"]
print(oob_err)
## Plot the train model
plot(train_model)
# Add a legend since it doesn't have one by default
legend(x = "right",
legend = colnames(err),
fill = 1:ncol(err))
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
## Create the dataframe for sample file
colnames(prob_prediction)<- c("Happening", "Not Happening")
sample<-cbind(test$id,prob_prediction$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
any(is.na(sample))
any(is.na(test[,factor_list_test]))
any(is.na(test$age))
any(is.na(test$bmi))
any(is.na(test$avg_glucose_level))
test$bmi[is.na(test$bmi)] <- mean(test$bmi,na.rm=T)
any(is.na(test))
rm(train_model)
rm(prob_prediction)
rm(sample)
## Set the seed
set.seed(1)
## Train the model
train_model<- randomForest(stroke~.,data = train)
## Print the model output
print(train_model)
## Evaluate Out of Bag error
err<- train_model$err.rate
## Look at the final OOB rate
oob_err<-err[nrow(err), "OOB"]
print(oob_err)
## Plot the train model
plot(train_model)
# Add a legend since it doesn't have one by default
legend(x = "right",
legend = colnames(err),
fill = 1:ncol(err))
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
## Create the dataframe for sample file
colnames(prob_prediction)<- c("Happening", "Not Happening")
sample<-cbind(test$id,prob_prediction$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
any(is.na(sample))
write.csv(sample,"sample_submission.csv")
prob_prediction
names(test)
names(train)
model_gbm<-gbm(stroke~., distribution = "bernoulli", data = train, n.trees = 500)
library(gbm)
model_gbm<-gbm(stroke~., distribution = "bernoulli", data = train, n.trees = 500)
prob_gbm<-as.data.frame(predict(object = model_gbm,
newdata = test,
type = "prob"))
prob_gbm<-as.data.frame(predict(object = model_gbm,
newdata = test,
n.trees = 500,
type = "prob"))
prob_gbm<-as.data.frame(predict(object = model_gbm,
newdata = test,
n.trees = 500,
type = "link"))
prob_gbm
model_gbm
prob_gbm<-as.data.frame(predict(object = model_gbm,
newdata = test,
n.trees = 500,
type = "response"))
prob_gbm
prob_gbm<-predict(object = model_gbm,
newdata = test,
n.trees = 500,
type = "response")
prob_gbm
prob_gbm<-predict.gbm(object = model_gbm,
newdata = test,
n.trees = 500,
type = "response")
prob_gbm
model_gbm<-gbm(stroke~., distribution = "bernoulli", data = train, n.trees = 10000)
model_gbm<-gbm(stroke~., distribution = "bernoulli", data = train, n.trees = 1000)
prob_gbm<-predict.gbm(object = model_gbm,
newdata = test,
n.trees = 1000,
type = "response")
prob_gbm
prob_gbm<-predict.gbm(object = model_gbm,
newdata = test,
n.trees = 1000,
type = "prob")
prob_gbm<-predict.gbm(object = model_gbm,
newdata = test,
n.trees = 1000,
type = "link")
prob_gbm
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "response"))
prob_prediction
colnames(prob_prediction)<- c("Happening", "Not Happening")
sample<-cbind(test$id,prob_prediction$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
any(is.na(sample))
prob_prediction
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
prob_prediction
train_model
train_model<- glm(stroke~.,data = train)
train_model<- glm(stroke~.,data = train, family = binomial("logit"))
print(train_model)
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "response"))
prob_prediction
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "link"))
prob_prediction
train_model<- randomForest(stroke~.,data = train)
prob_prediction<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
prob_prediction
mean(prob_prediction)
mean(prob_prediction$`0`)
colnames(prob_prediction)<- c("Not Happening", "Happening")
sample<-cbind(test$id,prob_prediction$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
any(is.na(sample))
write.csv(sample,"sample_submission.csv")
train_model$importance
train_model$ntree
train_model$localImportance
train_model$importanceSD
n<-nrow(train)
n
index<-1:n
index
shuffled <- train[sample(n),]
shuffled
train_1<- shuffled[1:round(0.7 * n),]
validation<- shuffled[(round(0.7 * n) + 1):n,]
nrow(train_1)
nrow(validation)
Model_RF<- randomForest(stroke~.,data = train_1)
print(Model_RF)
err
head(err)
print(Model_RF)
Model_RF<- randomForest(default~.,data = train_1)
Model_RF<- randomForest(formula = default~.,data = train_1)
Model_RF<- randomForest(stroke~.,data = train_1)
print(Model_RF)
err<-Model_RF$err.rate
## Find the last row of err
err<-err[nrow(err),"OOB"]
err
plot(Model_RF)
class_prediction<- predict(object = Model_RF,
newdata = validation,
type = "prob")
library(caret)
cm<- confusionMatrix(data = class_prediction, reference = validation$stroke)
class_prediction
any(is.na(train_1))
class_prediction<- predict(object = Model_RF,
newdata = validation,
type = "class")
class_prediction
cm<- confusionMatrix(data = class_prediction, reference = validation$stroke)
install.packages("e1071")
cm<- confusionMatrix(data = class_prediction, reference = validation$stroke)
cm
auc(validation$stroke, prob_prediction_Model_RF)
library(ROCR)
auc(validation$stroke, prob_prediction_Model_RF)
library(perf)
install.packages("perf")
?auc()
install.packages("AUC")
library(AUC)
auc(validation$stroke, prob_prediction_Model_RF)
auc(validation$stroke, class_prediction)
mtry <- seq(4, ncol(train_1) * 0.8, 2)
mtry
ncol(train_1)
12*0.8
seq(4,9.6,2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(train_1) * c(0.7, 0.8)
sampsize
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
hyper_grid
oob_err <- c()
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- randomForest(stroke ~ .,
data = train_1,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
sampsize = hyper_grid$sampsize[i])
# Store OOB error for the model
oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
optimal_grid_options<-print(hyper_grid[opt_i,])
optimal_grid_options
Model_RF_1<-randomForest(stroke~., data = train_1, mtry = 4, nodesize = 3, sampsize = 21266)
prob_prediction_Model_RF_1<- predict(object = Model_RF_1,
newdata = validation,
type = "prob")
prob_prediction_Model_RF_1
class_prediction<- predict(object = Model_RF_1,
newdata = validation,
type = "class")
cm<- confusionMatrix(data = class_prediction_1, reference = validation$stroke)
class_prediction_1<- predict(object = Model_RF_1,
newdata = validation,
type = "class")
cm<- confusionMatrix(data = class_prediction_1, reference = validation$stroke)
cm
auc(validation$stroke,class_prediction_1[,2])
auc(actual = validation$stroke,predicted = class_prediction_1[,2])
auc(actual = validation$stroke, predicted = class_prediction_1[,2])
auc(actual = validation$stroke, predicted = prob_prediction_Model_RF_1[,1])
auc(validation$stroke, prob_prediction_Model_RF_1[,1])
library(pROC)
install.package("pROC")
install.packages("pROC")
library(pROC)
auc(validation$stroke, prob_prediction_Model_RF_1)
auc(validation$stroke, class_prediction_Model_1)
auc(validation$stroke, class_prediction_1)
class_prediction_1
auc(validation$stroke, class_prediction_1[,1])
auc(validation$stroke, class_prediction_1[,2])
auc(validation$stroke, class_prediction_1[1])
validation$stroke
nro(validation)
nrow(validation)
roc_obj<- roc(validation$stroke, class_prediction_1)
roc_obj<- roc(validation$stroke, class_prediction_1[2,])
class_prediction_1<- as.data.frame(predict(object = Model_RF_1,
newdata = validation,
type = "class"))
head(class_prediction_1)
roc_obj<- roc(validation$stroke, class_prediction_1[,2])
colnames(class_prediction_1)<- c("id", "stroke")
ncol(class_prediction_1)
class_prediction_1
mtry <- seq(4, ncol(train) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(train) * c(0.7, 0.8)
# Create a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
# Create an empty vector to store OOB error values
oob_err <- c()
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- randomForest(stroke ~ .,
data = train,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
sampsize = hyper_grid$sampsize[i])
# Store OOB error for the model
oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
optimal_grid_options<-print(hyper_grid[opt_i,])
train_model_1<-randomForest(stroke~., data = train, mtry = 4, nodesize = 3, sampsize = 30380)
prob_prediction_train_model_1<- predict(object = train_model_1,
newdata = validation,
type = "prob")
rm(prob_prediction_train_model_1)
prob_prediction_1<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
colnames(prob_prediction_1)<- c("Not Happening", "Happening")
sample<-cbind(test$id,prob_prediction_1$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
any(is.na(sample))
write.csv(sample,"sample_submission.csv")
source('D:/Documents/Hackathon/Mckinsey_Healthcare_April_2018/healthcare.R')
library(corrplot)
str(train)
data_cor<-train[,c(2,3,4,5,6,7,8,9,10,11,12)]
cor_mat<-cor(data_cor)
data_cor<-as.numeric(data_cor)
class(data_cor)
data_cor<-as.numeric(data_cor[,factor_list])
data_cor[,colnames(data_cor)]<-lapply(data_cor[,colnames(data_cor)],numeric)
data_cor[,2:12]<-lapply(data_cor[,2:12],numeric)
names(data_cor)
data_cor[,1:11]<-lapply(data_cor[,1:11],numeric)
data_cor[,1:11]
lapply(data_cor[,1:11],numeric)
data_cor[,1:11]<-lapply(data_cor[,1:11], as.numeric)
str(data_cor)
cor_mat<-cor(data_cor)
corrplot(cor_mat)
cor(data_cor$smoking_status,data_cor$stroke)
cor(data_cor$age,data_cor$stroke)
cor(train)
corrplot(train)
importance(train_model)
names(train_model)
set.seed(1)
res <- tuneRF(x = subset(train_model, select = -stroke),
y = train_model[,17],
ntreeTry = 500)
x = subset(train, select = -stroke
0
x = subset(train, select = -stroke)
x
names(train)
set.seed(1)
res <- tuneRF(x = subset(train, select = -stroke),
y = train[,12],
ntreeTry = 500)
# Look at results
print(res)
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)
mtry <- seq(2, ncol(train) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(train) * c(0.7, 0.8)
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
hyper_grid
train_model_1<-randomForest(stroke~., data = train, mtry = 2, nodesize = 3, sampsize = 30380)
prob_prediction_1<- as.data.frame(predict(object = train_model,
newdata = test,
type = "prob"))
varImpPlot(train_model)
varImpPlot(train_model_1)
names(train)
train_1 = train[,c(-6,-4,-2,-8)]
train_model_2<-randomForest(stroke~.,data = train_1, mtry = 2, nodesize = 3, sampsize = 30380)
prob_prediction_1<- as.data.frame(predict(object = train_model_2,
newdata = test,
type = "prob"))
colnames(prob_prediction_1)<- c("Not Happening", "Happening")
sample<-cbind(test$id,prob_prediction_1$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
any(is.na(sample))
write.csv(sample,"sample_submission.csv")
varImpPlot(train_model_2)
names(train)
train_1 = train[,c(-6,-4,-2,-8,-1)]
train_model_2<-randomForest(stroke~.,data = train_1, mtry = 2, nodesize = 3, sampsize = 30380)
prob_prediction_1<- as.data.frame(predict(object = train_model_2,
newdata = test,
type = "prob"))
colnames(prob_prediction_1)<- c("Not Happening", "Happening")
colnames(sample)<- c("id", "stroke")
head(sample)
varImpPlot(train_model_2)
any(is.na(sample))
write.csv(sample,"sample_submission.csv")
source('D:/Documents/Hackathon/Mckinsey_Healthcare_April_2018/healthcare.R')
names(train)
names(train_1)
print(train_model_2)
err<-Model_RF$err.rate
err<-err[nrow(err),"OOB"]
plot(err)
plot(model_2)
plot(train_model_2)
set.seed(1)
res <- tuneRF(x = subset(train_1, select = -stroke),
y = train[,12],
ntreeTry = 50)
print(res)
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)
mtry <- seq(1, ncol(train) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(train) * c(0.7, 0.8)
# Create a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
oob_err <- c()
for (i in 1:nrow(hyper_grid)) {
# Train a Random Forest model
model <- randomForest(stroke ~ .,
data = train_1,
mtry = hyper_grid$mtry[i],
nodesize = hyper_grid$nodesize[i],
sampsize = hyper_grid$sampsize[i],
ntrees = 50)
# Store OOB error for the model
oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}
# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
optimal_grid_options<-print(hyper_grid[opt_i,])
train_model_2<-randomForest(stroke~.,data = train_1, mtry = 1, nodesize = 3, sampsize = 30380)
plot(train_model_2)
prob_prediction_1<- as.data.frame(predict(object = train_model_2,
newdata = test,
type = "prob"))
## Create the dataframe for sample file
colnames(prob_prediction)<- c("Not Happening", "Happening")
sample<-cbind(test$id,prob_prediction$Happening)
colnames(sample)<- c("id", "stroke")
head(sample)
any(is.na(sample))
write.csv(sample,"sample_submission.csv")
